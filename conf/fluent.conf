@include healthcheck.conf
@include systemd.conf
@include docker.conf

# Only process monitoring_events of content_type 'Annotations'
<match docker.annotations.**>
  
  #In order to redirect annotation logs to S3 and Kinesis we need to copy them

  @type copy
  deep_copy true
  <store>
    @type kinesis_streams
    stream_name "#{ENV['KINESIS_STREAM']}"
    aws_key_id "#{ENV['S3_ACCESS_KEY']}"
    aws_sec_key "#{ENV['S3_SECRET_ACCESS_KEY']}"
    # Recommended throughput settings
    <buffer>
      flush_mode interval
      flush_interval 2s
    </buffer>
    <instance_profile_credentials>
      ip_address 169.254.169.254
      port       80b
    </instance_profile_credentials>
  </store>
  
  <store>
    @type relabel
    @label @annotations
  </store>
</match>

#### Annotations Filters###################################################
<label @annotations>
  <filter docker.annotations.**>
     @type parser
    key_name MESSAGE

     reserve_data true
    emit_invalid_record_to_error false
    
    <parse>
      # Parse the MESSAGE field as json, and merge the new fields into the log message
      @type json
    </parse>
  </filter>

  <filter docker.annotations.**>
     @type       record_transformer
    enable_ruby true
    remove_keys ["CONTAINER_TAG","SYSLOG_IDENTIFIER"]
  </filter>

  <filter docker.annotations.**>
   @type       record_transformer
   enable_ruby true
   <record>
      event            ${record}
      time             ${record["_SOURCE_REALTIME_TIMESTAMP"]}
   </record>
  </filter>

  <filter docker.annotations.**>
   @type       record_transformer
   enable_ruby true
   renew_record true
   keep_keys event, time
  </filter>

  #### End Annotations Filter ###################################

  <match docker.annotations.**>

    @type s3
    @log_level info
    aws_key_id "#{ENV['S3_ACCESS_KEY']}"
    aws_sec_key "#{ENV['S3_SECRET_ACCESS_KEY']}"
    s3_bucket "#{ENV['BUCKET_NAME']}"
    s3_region "#{ENV['AWS_REGION']}"
    s3_object_key_format %{path}/fluentd-%{index}
    path "#{ENV['ENVIRONMENT_NAME']}"
    store_as text

    <buffer>
     @type file
     path /var/log/fluentd-buffers/s3-annotations.buffer
     timekey 3600 # 1 hour partition
     timekey_wait 5m
     timekey_use_utc true # use utc
     flush_mode interval
     flush_interval 2s
     chunk_limit_size 256m 
     flush_thread_count 8
    </buffer>

    <format>
      @type json
    </format>

    <instance_profile_credentials>
     ip_address 169.254.169.254
     port       80b
    </instance_profile_credentials>
  
  </match>
</label>

#Match all remaining logs, and output them to the fluentd /dev/null equivalent for now
<match docker.source>
  #@type null
  @type s3
  @log_level info
   aws_key_id "#{ENV['S3_ACCESS_KEY']}"
   aws_sec_key "#{ENV['S3_SECRET_ACCESS_KEY']}"
   s3_bucket "#{ENV['BUCKET_NAME']}"
   s3_region "#{ENV['AWS_REGION']}"
   s3_object_key_format %{path}/fluentd-_%{time_slice}%{hex_random}_%{hostname}
   path "#{ENV['ENVIRONMENT_NAME']}"
   store_as text

   <buffer>
     @type memory
     #path /var/log/fluentd-buffers/s3.buffer
     #timekey 3600 # 1 hour partition
     #timekey_wait 5m
     #timekey_use_utc true # use utc
     flush_mode interval
     flush_interval 2s
     chunk_limit_size 256m 
     flush_thread_count 8
   </buffer>

   <format>
    @type json
   </format>

   <instance_profile_credentials>
     ip_address 169.254.169.254
     port       80b
   </instance_profile_credentials>
</match>

#<match blacklisted.*>
#  @type null
#</match>