@include healthcheck.conf
@include systemd.conf
@include docker.conf

#### OUTPUT SECTION #####

# Only process monitoring_events of content_type 'Annotations' to Kinesis
<match docker.annotations>  
  # In order to redirect annotation logs to S3 and Kinesis we need to copy them
  @type copy
  deep_copy true
  <store>
    @type kinesis_streams
    stream_name "#{ENV['KINESIS_STREAM']}"
    aws_key_id "#{ENV['S3_ACCESS_KEY']}"
    aws_sec_key "#{ENV['S3_SECRET_ACCESS_KEY']}"
    <buffer>
      flush_mode interval
      flush_interval 2s
    </buffer>
    <instance_profile_credentials>
      ip_address 169.254.169.254
      port       80b
    </instance_profile_credentials>
  </store>
  # Relabel annotations and prepare for S3
  <store>
    @type relabel
    @label @annotations
  </store>
</match>

# Send annotation monitoring events to S3 (SPLUNK) as well
<label @annotations>
  ### Annotations Filters###################################################
  <filter docker.annotations>
    @type  record_transformer
    enable_ruby true
    <record>
       event            ${record}
       time             ${record["_SOURCE_REALTIME_TIMESTAMP"]}
    </record>
  </filter>
  <filter docker.annotations> 
    @type  record_transformer
    enable_ruby true
    renew_record true
    keep_keys event, time
  </filter>
  ### End Annotations Filter ###################################

  <match docker.annotations>
   @type s3
   @log_level info
   aws_key_id "#{ENV['S3_ACCESS_KEY']}"
   aws_sec_key "#{ENV['S3_SECRET_ACCESS_KEY']}"
   s3_bucket "#{ENV['BUCKET_NAME']}"
   s3_region "#{ENV['AWS_REGION']}"
   s3_object_key_format %{path}/fluentd-%{index}
   path "#{ENV['ENVIRONMENT_NAME']}"
   store_as text
  
   <buffer>
     @type file_single
     path /var/log/fluentd-buffers/s3-annotations.buffer
     timekey 3600 # 1 hour partition
     timekey_wait 5m
     timekey_use_utc true # use utc
     flush_mode interval
     flush_interval 10s
     chunk_limit_size 32m  
     flush_thread_count 16
   </buffer>

   <format>
     @type json
   </format>

   <instance_profile_credentials>
    ip_address 169.254.169.254
    port       80b
   </instance_profile_credentials>
  
  </match>
</label>

#Match all remaining logs, and output them to the fluentd /dev/null equivalent for now
<match docker.**>
  @type s3
  @log_level info
  aws_key_id "#{ENV['S3_ACCESS_KEY']}"
  aws_sec_key "#{ENV['S3_SECRET_ACCESS_KEY']}"
  s3_bucket "#{ENV['BUCKET_NAME']}"
  s3_region "#{ENV['AWS_REGION']}"
  s3_object_key_format %{path}/fluentd-_%{time_slice}%{hex_random}_%{hostname}
  path "#{ENV['ENVIRONMENT_NAME']}"
  store_as text

  <buffer>
     @type file_single
     path /var/log/fluentd-buffers/s3.buffer
     timekey 3600 # 1 hour partition
     timekey_wait 5m
     timekey_use_utc true # use utc
     flush_mode interval
     flush_interval 10s
     chunk_limit_size 32m 
     flush_thread_count 16
  </buffer>

  <format>
    @type json
  </format>

  <instance_profile_credentials>
    ip_address 169.254.169.254
    port       80b
  </instance_profile_credentials>
</match>

# Enable section if Fluentd troubleshooting is needed, set log level to debug on each input/match/output rule
#<label @FLUENT_LOG>
#  <match fluent.*>
#    @type stdout
#  </match>
#</label>

# Drop all events tagged as blacklisted.*
<match blacklisted.*>
  @type null
</match>